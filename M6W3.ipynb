{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e714f47-2677-4eaf-83fd-168cebaff926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3600000 entries, 0 to 3599999\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Dtype \n",
      "---  ------    ----- \n",
      " 0   score     object\n",
      " 1   review    object\n",
      " 2   n_tokens  int64 \n",
      " 3   language  int32 \n",
      "dtypes: int32(1), int64(1), object(2)\n",
      "memory usage: 96.1+ MB\n",
      "None\n",
      "\n",
      "Test Set Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400000 entries, 0 to 399999\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   score     400000 non-null  object\n",
      " 1   review    400000 non-null  object\n",
      " 2   n_tokens  400000 non-null  int64 \n",
      " 3   language  400000 non-null  int32 \n",
      "dtypes: int32(1), int64(1), object(2)\n",
      "memory usage: 10.7+ MB\n",
      "None\n",
      "\n",
      "Missing values in Training Set:\n",
      "score       0\n",
      "review      0\n",
      "n_tokens    0\n",
      "language    0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in Test Set:\n",
      "score       0\n",
      "review      0\n",
      "n_tokens    0\n",
      "language    0\n",
      "dtype: int64\n",
      "\n",
      "Unique labels in Training Set:\n",
      "['2' '1']\n",
      "\n",
      "Unique labels in Test Set:\n",
      "['2' '1']\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 26.9 GiB for an array with shape (3600000, 1002) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16184\\37605406.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;31m# Add the other features.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m \u001b[0mX_train_transformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_train_transformed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'n_tokens'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'language'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[0mX_test_transformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_test_transformed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'n_tokens'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'language'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mhstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mhstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    343\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 26.9 GiB for an array with shape (3600000, 1002) and data type int64"
     ]
    }
   ],
   "source": [
    "# M6W3.\n",
    "\n",
    "\n",
    "# Import required libraries.\n",
    "import bz2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from langdetect import detect\n",
    "\n",
    "\n",
    "# Load and decode train file.\n",
    "train_file = bz2.BZ2File(r\"C:\\Users\\deon.archary\\OneDrive - TO70\\Bureaublad\\M6W3\\train.ft.txt.bz2\", 'rb')\n",
    "lines = [x.decode('utf-8') for x in train_file.readlines()]\n",
    "\n",
    "\n",
    "# Extract reviews and labels.\n",
    "score_review_list = [l.replace('__label__', '').split(' ', 1) for l in lines]\n",
    "df_train = pd.DataFrame(score_review_list, columns=['score', 'review'])\n",
    "\n",
    "\n",
    "# Load and decode test file.\n",
    "test_file = bz2.BZ2File(r\"C:\\Users\\deon.archary\\OneDrive - TO70\\Bureaublad\\M6W3\\test.ft.txt.bz2\", 'rb')\n",
    "lines_test = [x.decode('utf-8') for x in test_file.readlines()]\n",
    "\n",
    "\n",
    "# Extract reviews and labels from test data.\n",
    "score_review_list_test = [l.replace('__label__', '').split(' ', 1) for l in lines_test]\n",
    "df_test = pd.DataFrame(score_review_list_test, columns=['score', 'review'])\n",
    "\n",
    "\n",
    "# Add n_tokens feature.\n",
    "df_train['n_tokens'] = df_train['review'].apply(lambda x: len(x.split()))\n",
    "df_test['n_tokens'] = df_test['review'].apply(lambda x: len(x.split()))\n",
    "\n",
    "\n",
    "# Add language feature\n",
    "def detect_language(x):\n",
    "    try:\n",
    "        if not x.strip():\n",
    "            return 'en'\n",
    "        return detect(x)\n",
    "    except:\n",
    "        return 'en'\n",
    "\n",
    "df_train['language'] = df_train['review'].apply(detect_language)\n",
    "df_test['language'] = df_test['review'].apply(detect_language)\n",
    "\n",
    "\n",
    "# Convert language feature to binary.\n",
    "df_train['language'] = np.where(df_train['language'] == 'en', 1, 0)\n",
    "df_test['language'] = np.where(df_test['language'] == 'en', 1, 0)\n",
    "\n",
    "\n",
    "# Basic EDA, Missing Data and Label checks.\n",
    "print(\"Training Set Information:\")\n",
    "print(df_train.info())\n",
    "print(\"\\nTest Set Information:\")\n",
    "print(df_test.info())\n",
    "\n",
    "\n",
    "print(\"\\nMissing values in Training Set:\")\n",
    "print(df_train.isnull().sum())\n",
    "print(\"\\nMissing values in Test Set:\")\n",
    "print(df_test.isnull().sum())\n",
    "\n",
    "\n",
    "print(\"\\nUnique labels in Training Set:\")\n",
    "print(df_train['score'].unique())\n",
    "print(\"\\nUnique labels in Test Set:\")\n",
    "print(df_test['score'].unique())\n",
    "\n",
    "\n",
    "# Encode labels.\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(df_train['score'])\n",
    "y_test = le.transform(df_test['score'])\n",
    "\n",
    "\n",
    "# Bag of words.\n",
    "vectorizer = CountVectorizer(max_features=1000)\n",
    "X_train_transformed = vectorizer.fit_transform(df_train['review'])\n",
    "X_test_transformed = vectorizer.transform(df_test['review'])\n",
    "\n",
    "\n",
    "# Add the other features.\n",
    "X_train_transformed = np.hstack([X_train_transformed.toarray(), df_train[['n_tokens', 'language']].values])\n",
    "X_test_transformed = np.hstack([X_test_transformed.toarray(), df_test[['n_tokens', 'language']].values])\n",
    "\n",
    "\n",
    "# Check the number of columns.\n",
    "print(f\"\\nNumber of columns in the transformed Training Set: {X_train_transformed.shape[1]}\")\n",
    "print(f\"Number of columns in the transformed Test Set: {X_test_transformed.shape[1]}\")\n",
    "\n",
    "\n",
    "# Train model.\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train_transformed, y_train)\n",
    "\n",
    "\n",
    "# Predict and evaluate.\n",
    "predictions_train = model.predict(X_train_transformed)\n",
    "predictions_test = model.predict(X_test_transformed)\n",
    "\n",
    "\n",
    "print(f'Training Accuracy: {accuracy_score(y_train, predictions_train)}')\n",
    "print(f'Training Confusion Matrix:\\n{confusion_matrix(y_train, predictions_train)}')\n",
    "\n",
    "\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, predictions_test)}')\n",
    "print(f'Test Confusion Matrix:\\n{confusion_matrix(y_test, predictions_test)}')\n",
    "\n",
    "\n",
    "# Evaluate feature importance.\n",
    "feature_importances = model.feature_importances_\n",
    "features = vectorizer.get_feature_names() + ['n_tokens', 'language']\n",
    "\n",
    "\n",
    "feature_importances_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "print(\"\\nFeature Importances:\")\n",
    "print(feature_importances_df.sort_values(by='Importance', ascending=False))\n",
    "\n",
    "\n",
    "# - The sentiment of the review is done through a classification model. In this case, I used a Random \n",
    "#   Forest Classifier.\n",
    "\n",
    "# - I transformed the language feature to a binary feature, where 1 indicates English and 0 indicates other \n",
    "#   languages.\n",
    "\n",
    "# - The Count Vectorizer is used to convert the review text into a numerical format that can be used for \n",
    "#   modeling. The result is a bag of words representation of the review. This is combined with the other \n",
    "#   created features, 'n_tokens' and 'language', for the training and test sets.\n",
    "\n",
    "# - The model is trained using the transformed training set and evaluated using the transformed test set. \n",
    "#   The accuracy score and confusion matrix are used for performance evaluation. The accuracy score is used\n",
    "#   for classification tasks and gives a understanding of how well the model performs. The confusion matrix \n",
    "#   gives better details of the model's performance.\n",
    "\n",
    "#   I do not have enough memory available, so I have reduced the number of features to 500 in version 2 \n",
    "#   below. There is also the possibility of using different approachs to processing the data when faced\n",
    "#   with limited memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d86fe752-a314-43b1-8f1f-bf9d04f941e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3600000 entries, 0 to 3599999\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Dtype \n",
      "---  ------    ----- \n",
      " 0   score     object\n",
      " 1   review    object\n",
      " 2   n_tokens  int64 \n",
      " 3   language  int32 \n",
      "dtypes: int32(1), int64(1), object(2)\n",
      "memory usage: 96.1+ MB\n",
      "None\n",
      "\n",
      "Test Set Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400000 entries, 0 to 399999\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   score     400000 non-null  object\n",
      " 1   review    400000 non-null  object\n",
      " 2   n_tokens  400000 non-null  int64 \n",
      " 3   language  400000 non-null  int32 \n",
      "dtypes: int32(1), int64(1), object(2)\n",
      "memory usage: 10.7+ MB\n",
      "None\n",
      "\n",
      "Missing values in Training Set:\n",
      "score       0\n",
      "review      0\n",
      "n_tokens    0\n",
      "language    0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in Test Set:\n",
      "score       0\n",
      "review      0\n",
      "n_tokens    0\n",
      "language    0\n",
      "dtype: int64\n",
      "\n",
      "Unique labels in Training Set:\n",
      "['2' '1']\n",
      "\n",
      "Unique labels in Test Set:\n",
      "['2' '1']\n",
      "\n",
      "Number of columns in the transformed Training Set: 502\n",
      "Number of columns in the transformed Test Set: 502\n",
      "Training Accuracy: 0.9998530555555556\n",
      "Training Confusion Matrix:\n",
      "[[1799600     400]\n",
      " [    129 1799871]]\n",
      "Test Accuracy: 0.8454675\n",
      "Test Confusion Matrix:\n",
      "[[171108  28892]\n",
      " [ 32921 167079]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deon.archary\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importances:\n",
      "        Feature  Importance\n",
      "272         not    0.050927\n",
      "166       great    0.046026\n",
      "456       waste    0.018608\n",
      "234        love    0.016428\n",
      "45         best    0.015982\n",
      "..          ...         ...\n",
      "3    absolutely    0.000437\n",
      "63       camera    0.000413\n",
      "304       phone    0.000410\n",
      "427         toy    0.000378\n",
      "501    language    0.000251\n",
      "\n",
      "[502 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# M6W3.\n",
    "\n",
    "# Version 2 for limited memory, features are limited to 500 instead of 1000.\n",
    "\n",
    "# Import required libraries.\n",
    "import bz2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from langdetect import detect\n",
    "\n",
    "\n",
    "# Load and decode train file.\n",
    "train_file = bz2.BZ2File(r\"C:\\Users\\deon.archary\\OneDrive - TO70\\Bureaublad\\M6W3\\train.ft.txt.bz2\", 'rb')\n",
    "lines = [x.decode('utf-8') for x in train_file.readlines()]\n",
    "\n",
    "\n",
    "# Extract reviews and labels.\n",
    "score_review_list = [l.replace('__label__', '').split(' ', 1) for l in lines]\n",
    "df_train = pd.DataFrame(score_review_list, columns=['score', 'review'])\n",
    "\n",
    "\n",
    "# Load and decode test file.\n",
    "test_file = bz2.BZ2File(r\"C:\\Users\\deon.archary\\OneDrive - TO70\\Bureaublad\\M6W3\\test.ft.txt.bz2\", 'rb')\n",
    "lines_test = [x.decode('utf-8') for x in test_file.readlines()]\n",
    "\n",
    "\n",
    "# Extract reviews and labels from test data.\n",
    "score_review_list_test = [l.replace('__label__', '').split(' ', 1) for l in lines_test]\n",
    "df_test = pd.DataFrame(score_review_list_test, columns=['score', 'review'])\n",
    "\n",
    "\n",
    "# Add n_tokens feature.\n",
    "df_train['n_tokens'] = df_train['review'].apply(lambda x: len(x.split()))\n",
    "df_test['n_tokens'] = df_test['review'].apply(lambda x: len(x.split()))\n",
    "\n",
    "\n",
    "# Add language feature.\n",
    "def detect_language(x):\n",
    "    try:\n",
    "        if not x.strip():\n",
    "            return 'en'\n",
    "        return detect(x)\n",
    "    except:\n",
    "        return 'en'\n",
    "\n",
    "df_train['language'] = df_train['review'].apply(detect_language)\n",
    "df_test['language'] = df_test['review'].apply(detect_language)\n",
    "\n",
    "\n",
    "# Convert language feature to binary.\n",
    "df_train['language'] = np.where(df_train['language'] == 'en', 1, 0)\n",
    "df_test['language'] = np.where(df_test['language'] == 'en', 1, 0)\n",
    "\n",
    "\n",
    "# Basic EDA, Missing Data and Label checks.\n",
    "print(\"Training Set Information:\")\n",
    "print(df_train.info())\n",
    "print(\"\\nTest Set Information:\")\n",
    "print(df_test.info())\n",
    "\n",
    "\n",
    "print(\"\\nMissing values in Training Set:\")\n",
    "print(df_train.isnull().sum())\n",
    "print(\"\\nMissing values in Test Set:\")\n",
    "print(df_test.isnull().sum())\n",
    "\n",
    "\n",
    "print(\"\\nUnique labels in Training Set:\")\n",
    "print(df_train['score'].unique())\n",
    "print(\"\\nUnique labels in Test Set:\")\n",
    "print(df_test['score'].unique())\n",
    "\n",
    "\n",
    "# Encode labels.\n",
    "le = LabelEncoder()\n",
    "y_train = le.fit_transform(df_train['score'])\n",
    "y_test = le.transform(df_test['score'])\n",
    "\n",
    "\n",
    "# Bag of words.\n",
    "vectorizer = CountVectorizer(max_features=500)\n",
    "X_train_transformed = vectorizer.fit_transform(df_train['review'])\n",
    "X_test_transformed = vectorizer.transform(df_test['review'])\n",
    "\n",
    "\n",
    "# Add the other features.\n",
    "X_train_transformed = np.hstack([X_train_transformed.toarray(), df_train[['n_tokens', 'language']].values])\n",
    "X_test_transformed = np.hstack([X_test_transformed.toarray(), df_test[['n_tokens', 'language']].values])\n",
    "\n",
    "\n",
    "# Check the number of columns.\n",
    "print(f\"\\nNumber of columns in the transformed Training Set: {X_train_transformed.shape[1]}\")\n",
    "print(f\"Number of columns in the transformed Test Set: {X_test_transformed.shape[1]}\")\n",
    "\n",
    "\n",
    "# Train model.\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train_transformed, y_train)\n",
    "\n",
    "\n",
    "# Predict and evaluate.\n",
    "predictions_train = model.predict(X_train_transformed)\n",
    "predictions_test = model.predict(X_test_transformed)\n",
    "\n",
    "\n",
    "print(f'Training Accuracy: {accuracy_score(y_train, predictions_train)}')\n",
    "print(f'Training Confusion Matrix:\\n{confusion_matrix(y_train, predictions_train)}')\n",
    "\n",
    "\n",
    "print(f'Test Accuracy: {accuracy_score(y_test, predictions_test)}')\n",
    "print(f'Test Confusion Matrix:\\n{confusion_matrix(y_test, predictions_test)}')\n",
    "\n",
    "\n",
    "# Evaluate feature importance.\n",
    "feature_importances = model.feature_importances_\n",
    "features = vectorizer.get_feature_names() + ['n_tokens', 'language']\n",
    "\n",
    "\n",
    "feature_importances_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "print(\"\\nFeature Importances:\")\n",
    "print(feature_importances_df.sort_values(by='Importance', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a9b936-75c6-4a85-a604-a1acfcb3d572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
